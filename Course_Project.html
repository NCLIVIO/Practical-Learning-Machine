Course Project: Practical Machine Learning
Natalia clivio

Saturday, August 24, 2014

Introduction
The objective of this analysis is to predict the manner in which six participants did the exercise. They performed the excercise in five different ways; one exactly follows the specification, and the other four follow the specification incorrect way. The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har

Data Preparation
It is important to prepare the data to obtain more accurate results.

#Load Data splitting
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
library(ISLR)

training <-read.csv("C:/Users/NataliaA/Desktop/CourseaR/pml-training.csv")
testing <-read.csv("C:/Users/NataliaA/Desktop/CourseaR/pml-testing.csv")

# remove near zero covariates
nsv <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, !nsv$nzv]

# remove variables with more than 80% missing values
nav <- sapply(colnames(training), function(x) if(sum(is.na(training[, x])) > 0.8*nrow(training)){return(T)}else{return(F)})
training <- training[, !nav]

# calculate correlations
cor <- abs(sapply(colnames(training[, -ncol(training)]), function(x) cor(as.numeric(training[, x]), as.numeric(training$classe), method = "spearman")))
summary(cor)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0015  0.0147  0.0526  0.1020  0.1390  0.9770
Plot the variables more correlationed with the classe variable in the training set data, are showed in the following figure:



The training set has 19622 samples and 57 potential predictors after filtering.

There doesnâ€™t seem to be any strong predictors that correlates with classe properly, thereby linear regression model is probably not suitable in this case. Random forest and Boosting algorithms may generate more robust predictions for our data.

Then the Random forest and Bosting algorithms are compared to determine which is more accurate and makes final predictions.

Random Forest
library(randomForest)
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
set.seed(123)
modFitrf <- train(classe ~ ., method = "rf", data = training, importance = TRUE, trControl = trainControl(method = "cv", number = 10))

modFitrf
## Random Forest 
## 
## 19622 samples
##    58 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 17660, 17661, 17659, 17660, 17658, 17660, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##   2     1         1      0.002        0.003   
##   40    1         1      2e-04        2e-04   
##   80    1         1      3e-04        4e-04   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 41.
The accurancy of the model is:

plot(modFitrf, ylim = c(0.95, 1))
 Figure 1

The random forests algorithm generated has a very accurate model with accuracy close to 1.

Boosting model
Fit model with boosting algorithm and 10-fold cross validation to predict classe with all other predictors.

#Boosing
set.seed(123)
modFitB <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))
## Loading required package: gbm
## Loading required package: survival
## Loading required package: splines
## 
## Attaching package: 'survival'
## 
## The following object is masked from 'package:caret':
## 
##     cluster
## 
## Loading required package: parallel
## Loaded gbm 2.1
## Loading required package: plyr
modFitB
## Stochastic Gradient Boosting 
## 
## 19622 samples
##    58 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 17660, 17661, 17659, 17659, 17658, 17660, ... 
## 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
##   1                  50       1         1      3e-04        4e-04   
##   1                  100      1         1      3e-04        4e-04   
##   1                  200      1         1      3e-04        4e-04   
##   2                  50       1         1      3e-04        4e-04   
##   2                  100      1         1      3e-04        4e-04   
##   2                  200      1         1      3e-04        4e-04   
##   3                  50       1         1      3e-04        4e-04   
##   3                  100      1         1      3e-04        4e-04   
##   3                  200      1         1      3e-04        4e-04   
## 
## Tuning parameter 'shrinkage' was held constant at a value of 0.1
## Accuracy was used to select the optimal model using  the largest value.
## The final values used for the model were n.trees = 50, interaction.depth
##  = 1 and shrinkage = 0.1.
The accurancy of the model is:

plot(modFitB, ylim = c(0.95, 1))
 Figure 2

The boosting algorithm generated a good model with accuracy = 0.997.Compared with the Random Forest, this model has less performance in terms of accuracy according to the figures 1 and 2.

Final Model and Predictions
According to the above analysis, the final model will be done with Random Forest. The final random forest model we can say:

Contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are: +raw_timestamp_part_1 +roll_belt +num_window +pitch_forearm +cvtd_timestamp

Estimated out of sample error rate for the random forests model is 0.04% as reported by the final model. Predict the test set and output results for automatic grader.

# final model
modFitrf$finalModel
## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 41
## 
##         OOB estimate of  error rate: 0.01%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 5580    0    0    0    0   0.0000000
## B    1 3796    0    0    0   0.0002634
## C    0    1 3421    0    0   0.0002922
## D    0    0    0 3216    0   0.0000000
## E    0    0    0    0 3607   0.0000000
# prediction
prediction <- as.character(predict(modFitrf, testing))
Predictions for the 20 test samples
Now we will apply the machine learning algorithm to predict 20 cases.

# write prediction files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(prediction)
